!pip install ultralytics transformers opencv-python torch torchvision torchaudio

import cv2
import os
import torch
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from ultralytics import YOLO
from google.colab import drive

# âœ… Google Drive ì—°ê²°
drive.mount('/content/drive')

# âœ… ê²½ë¡œ ì„¤ì • (Google Drive ë‚´ í”„ë¡œì íŠ¸ í´ë”ë¡œ ë³€ê²½)
PROJECT_DIR = "/content/drive/MyDrive/í”„ë¡œì íŠ¸/íŒŒì´/"  # âš¡ï¸ ì—¬ê¸°ë¥¼ ë„ˆì˜ í”„ë¡œì íŠ¸ í´ë” ê²½ë¡œë¡œ ë³€ê²½
FRAME_DIR = os.path.join(PROJECT_DIR, "frames")
RESULTS_DIR = os.path.join(PROJECT_DIR, "detections")
CAPTION_DIR = os.path.join(PROJECT_DIR, "captions")

# âœ… YOLO & BLIP ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
yolo_model = YOLO("yolov8n.pt")  # YOLOv8 ëª¨ë¸
device = "cuda" if torch.cuda.is_available() else "cpu"

# âœ… BLIP-2 ëª¨ë¸ ë³€ê²½ (ê²½ëŸ‰í™” ëª¨ë¸ ì‚¬ìš©)
processor = Blip2Processor.from_pretrained("Salesforce/blip2-flan-t5-xl")
blip_model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-flan-t5-xl",
    torch_dtype=torch.float16,  # ë©”ëª¨ë¦¬ ìµœì í™”
    low_cpu_mem_usage=True
).to(device)

# âœ… Google Drive ê²½ë¡œ ì„¤ì • (ì—¬ê¸°ì— ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•´ì•¼ í•¨)
FRAME_DIR = "/content/drive/MyDrive/í”„ë¡œì íŠ¸/íŒŒì´/frames"  # í”„ë ˆì„ ì´ë¯¸ì§€ í´ë”
RESULTS_DIR = "/content/drive/MyDrive/í”„ë¡œì íŠ¸/íŒŒì´/detections"  # íƒì§€ ê²°ê³¼ í´ë”
CAPTION_DIR = "/content/drive/MyDrive/í”„ë¡œì íŠ¸/íŒŒì´/captions"  # ì„¤ëª… ì €ì¥ í´ë”
os.makedirs(RESULTS_DIR, exist_ok=True)
os.makedirs(CAPTION_DIR, exist_ok=True)

def detect_objects_and_generate_caption(image_path):
    """YOLOë¡œ ê°ì²´ íƒì§€ + BLIP-2ë¡œ ì„¤ëª… ìƒì„±"""
    img = cv2.imread(image_path)

    # âœ… YOLO íƒì§€
    results = yolo_model(img)
    detected_objects = []

    for result in results:
        for box in result.boxes:
            class_id = int(box.cls[0])
            label = yolo_model.names[class_id]  # ê°ì²´ ì´ë¦„
            detected_objects.append(label)

    # âœ… BLIP-2 ì„¤ëª… ìƒì„±
    raw_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    inputs = processor(images=raw_image, return_tensors="pt").to(device)

    with torch.no_grad():
        generated_ids = blip_model.generate(**inputs, max_length=50)
        blip_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

    # âœ… ìµœì¢… ì„¤ëª… ìƒì„±
    combined_caption = f"íƒì§€ëœ ê°ì²´: {', '.join(detected_objects)}. ì„¤ëª…: {blip_caption}"

    # âœ… íƒì§€ ê²°ê³¼ ì‹œê°í™” ë° ì €ì¥
    img_with_boxes = results[0].plot()
    save_path = os.path.join(RESULTS_DIR, os.path.basename(image_path))
    cv2.imwrite(save_path, img_with_boxes)

    # âœ… ì„¤ëª… ì €ì¥
    caption_path = os.path.join(CAPTION_DIR, f"{os.path.basename(image_path)}.txt")
    with open(caption_path, "w", encoding="utf-8") as f:
        f.write(combined_caption)

    print(f"âœ… ë¶„ì„ ì™„ë£Œ: {os.path.basename(image_path)} -> {combined_caption}")

def process_all_images():
    """ëª¨ë“  í”„ë ˆì„ì„ ë¶„ì„í•˜ì—¬ ê°ì²´ íƒì§€ + ì„¤ëª… ìƒì„±"""
    for filename in os.listdir(FRAME_DIR):
        if filename.endswith(".jpg"):
            image_path = os.path.join(FRAME_DIR, filename)
            detect_objects_and_generate_caption(image_path)

if __name__ == "__main__":
    print("ğŸš€ YOLO ê°ì²´ íƒì§€ + BLIP-2 ì„¤ëª… ìƒì„± ì‹œì‘...")
    process_all_images()
    print("ğŸ‰ ëª¨ë“  í”„ë ˆì„ ë¶„ì„ ì™„ë£Œ!")